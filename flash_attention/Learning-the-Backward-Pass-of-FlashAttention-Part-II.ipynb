{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8bc731a-54e3-432e-9241-8055605fab2c",
   "metadata": {},
   "source": [
    "This IPython notebook contains the same content available via the [Link](https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part2/) with a full implementation of FlashAttention2 in Triton at the end of the notebook. However, for pedagogical purposes, it is a restricted version that cannot handle arbitrary sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "171c3ce1-98b3-4bbf-9178-6ebef1b05ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.runtime import driver\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bee4e17-b826-410b-bd3c-2141bd2c5011",
   "metadata": {},
   "outputs": [],
   "source": [
    "HAS_TENSOR_DESC = False\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4454869-571f-4a83-afe1-0f9b7cea9771",
   "metadata": {},
   "source": [
    "## Recap Forward and Backward Passes of Standard Attention\n",
    "\n",
    "In the first part of this tutorial, we walked through a detailed derivation of formulas used in the backward pass of standard attention. For ease of reference, they are included as follows:\n",
    "\n",
    "Given input sequences $ Q,\\: K,\\: V,\\: \\in \\mathbb{R}^{N\\times d} $ where $ N $ is the sequence length and $ d $ is the head dimension, the standard attention output $ O \\in \\mathbb{R}^{N\\times d} $ is calculated as follows (forward pass):\n",
    "\n",
    "$ S=QK^T \\in \\mathbb{R}^{N\\times N}\\quad P = \\operatorname{softmax}(S) \\quad O=PV \\in \\mathbb{R}^{N\\times d} $\n",
    "\n",
    "where $ \\operatorname{softmax} $ is applied row-wise.\n",
    "\n",
    "Then, assuming a scalar-valued loss function $ L $, by the backpropagation (i.e., reverse mode of automatic differentiation (AD)), the gradients of $ L $ w.r.t various inputs are calculated as follows:\n",
    "\n",
    "$ \\frac{\\partial L}{\\partial V} = P^T \\frac{\\partial L}{\\partial O} \\in \\mathbb{R}^{N\\times d} $\n",
    "\n",
    "$ \\frac{\\partial L}{\\partial P} = \\frac{\\partial L}{\\partial O} V^T \\in \\mathbb{R}^{N\\times N} $\n",
    "\n",
    "$ \\frac{\\partial L}{\\partial S} = \\operatorname{dsoftmax}(\\frac{\\partial L}{\\partial P}) \\in \\mathbb{R}^{N\\times N} $\n",
    "\n",
    "$ \\frac{\\partial L}{\\partial Q} = \\frac{\\partial L}{\\partial S}K \\in \\mathbb{R}^{N\\times d} $\n",
    "\n",
    "$ \\frac{\\partial L}{\\partial K} = \\frac{\\partial L}{\\partial S}^T Q \\in \\mathbb{R}^{N\\times d} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ac5eb-57b6-4f85-9e81-8fd48b54eaee",
   "metadata": {},
   "source": [
    "## The Implementation of the Backward Pass of FlashAttention in Triton\n",
    "\n",
    "![alt](./figures/flashattn-backward-pseudocode.png)\n",
    "\n",
    "To construct a direct correspondence between the mathematical equations and Triton code, we replace $ \\frac{\\partial L}{\\partial V} $ with $ dV $ with a slight abuse of notation (Please note that $ dV $ hereafter will no longer denote differential.), as in the backward pass, the matrix $ dV $ contains the gradient of scalar-valued loss function $ L $ w.r.t. $ V $, i.e., $ \\frac{\\partial L}{\\partial V} $. By applying similar replacements to all the other variables, we therefore obtain the following equations adopted in the FlashAttention2 paper:\n",
    "\n",
    "$ dV = P^T dO \\in \\mathbb{R}^{N\\times d} $\n",
    "\n",
    "$ dP = dOV^T \\in \\mathbb{R}^{N\\times N} $\n",
    "\n",
    "$ dS = \\operatorname{dsoftmax}(dP) \\in \\mathbb{R}^{N\\times N} $\n",
    "\n",
    "$ dQ = dSK \\in \\mathbb{R}^{N\\times d} $\n",
    "\n",
    "$ dK = dS^T Q \\in \\mathbb{R}^{N\\times d} $\n",
    "\n",
    "Another trick adopted in the FlashAttention paper is to simplify the calculation of $ dS = \\operatorname{dsoftmax}(dP) $, which is clearly derived in its appendix.\n",
    "\n",
    "For self-containedness, we include it as follows: (Please note $ dS_{i,:}, dP_{i,:} $ are all column vectors):\n",
    "\n",
    "$ dS_{i,:} = \\operatorname{dsoftmax}dP_{i,:} = (\\text{diag}(P_{i,:}) - P_{i,:}P_{i,:}^T)dP_{i,:} = P_{i,:} \\circ dP_{i,:} - \\left( P_{i,:}^T dP_{i,:} \\right) P_{i,:} $.\n",
    "\n",
    "where $ \\circ $ denotes Hadamard product (i.e., pointwise multiplication).\n",
    "\n",
    "Recall that $ dP = dO V^T$, written in element-wise form, $ dP_{ij} = do_i^T v_j $, (Please note $ do_j, v_j, k_j $ here denote the j-th row of $dO, V, K $ respectively, acting as a column vector.)\n",
    "\n",
    "Now, we can define \n",
    "\n",
    "$ D_i = P_{i,:}^T dP_{i,:} =  \\sum_j \\frac{\\exp(q_i^T k_j)}{L_i} do_i^T v_j = do_i^T \\sum_j \\frac{\\exp(q_i^T k_j)}{L_i} v_j = do_i^T o_i $\n",
    "\n",
    "then $ dS_{i,:} = P_{i,:} \\circ dP_{i,:} - D_i P_{i,:} $. (Readers seeking a comprehensive treatment (e.g., online-softmax in the forward pass) of FlashAttention are encouraged to refer to the original papers.)\n",
    "\n",
    "Now, we are in a position to dive into the Triton implementation of the backward pass of FlashAttention2.\n",
    "\n",
    "We assume readers have a basic familiarity with Triton. Otherwise, there are many excellent Triton tutorials, including the official ones, available online for your reference. In my view, figuring out how to move pointers to accurately access blocks of elements (i.e., load and store) in parallelly launched Triton programs is sufficient to grasp the core mechanisms of custom kernels developed in Triton.\n",
    "\n",
    "Instead of using `block pointer` defined by `make_block_ptr`, I find that directly working with N-dimensional pointers to access elements in memory is more straightforward. Furthermore, `mask` and `other` are implicitly broadcast to `pointer.shape` when using N-dimensional pointers, which can be conveniently used to handle boundary conditions.\n",
    "\n",
    "In the following, I will give some visual illustrations to facilitate your understanding of how `tl.load()` works, as there is no difference in read (`tl.load()`) and write (`tl.store()`) operations as long as their indexes are specified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e43dcdf1-ca4e-400d-8e1b-de013a52e049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4  5  6  7]\n",
      " [ 8  9 10 11 12 13 14 15]\n",
      " [16 17 18 19 20 21 22 23]\n",
      " [24 25 26 27 28 29 30 31]\n",
      " [32 33 34 35 36 37 38 39]\n",
      " [40 41 42 43 44 45 46 47]\n",
      " [48 49 50 51 52 53 54 55]\n",
      " [56 57 58 59 60 61 62 63]]\n",
      "[[ 0  1  2  3  4  5  6  7]\n",
      " [ 8  9 10 11 12 13 14 15]]\n"
     ]
    }
   ],
   "source": [
    "N = 8\n",
    "# Here, the content of the array is made intentionally to be the exact same as offsets relative to the base pointer.\n",
    "# Please note that in Triton language, all Pytorch tensors are implicitly converted to base pointers.\n",
    "\n",
    "A = np.arange(N * N).reshape(N, N)\n",
    "print(A)\n",
    "\n",
    "BLOCK_M = 2\n",
    "col_dim = N\n",
    "\n",
    "stride_row = N\n",
    "stride_col = 1\n",
    "\n",
    "offs_m = np.arange(BLOCK_M)[:, None] * stride_row + np.arange(col_dim)[None, :] * stride_col\n",
    "\n",
    "# N-dimensional tensors are stored contiguously in memory. \n",
    "# Otherwise, it would be recommended to call x.contiguous() before taking any tensor operations. \n",
    "# Here, we mimic this feature with np.ndarray.flatten.\n",
    "\n",
    "# illustrate loading tensors from memory\n",
    "print(A.flatten()[offs_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "101791d1-ba29-46d7-b24e-55643bbe2863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16 17 18 19 20 21 22 23]\n",
      " [24 25 26 27 28 29 30 31]]\n"
     ]
    }
   ],
   "source": [
    "# illustrate moving blocks step_size rows down, which will be used in the for loop to traverse over one dimension of a tensor.\n",
    "step_size = 2\n",
    "print(A.flatten()[offs_m + step_size * N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f19d51f2-2ee9-4221-8093-451cad9a919e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  8]\n",
      " [ 1  9]\n",
      " [ 2 10]\n",
      " [ 3 11]\n",
      " [ 4 12]\n",
      " [ 5 13]\n",
      " [ 6 14]\n",
      " [ 7 15]]\n",
      "[[16 24]\n",
      " [17 25]\n",
      " [18 26]\n",
      " [19 27]\n",
      " [20 28]\n",
      " [21 29]\n",
      " [22 30]\n",
      " [23 31]]\n"
     ]
    }
   ],
   "source": [
    "# illustrate loading tensors directly in its transposed version and moving blocks accordingly\n",
    "offs_m_T = np.arange(BLOCK_M)[None, :] * stride_row + np.arange(col_dim)[:, None] * stride_col\n",
    "print(A.flatten()[offs_m_T])\n",
    "print(A.flatten()[offs_m_T + step_size * N])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a4dee3-fa24-41e1-a1ae-850d40ba4c04",
   "metadata": {},
   "source": [
    "Here, we analyse a simplified version of FlashAttention (technically, FlashAttention2) adapted from the official Triton tutorial [Fused Attention](https://triton-lang.org/main/getting-started/tutorials/06-fused-attention.html#fused-attention), accounting for both the 'Causal' and 'Non-Causal' modes.\n",
    "\n",
    "The implementation of the backward pass of FlashAttention can be generally grouped into three stages:\n",
    "\n",
    "1. Calculate the matrix $ D $ first as a preprocessing step, where $ D_i = do_i^T o_i $, which corresponds to the variable `delta = torch.empty_like(M)`. Its size is `(Batch, Num_Heads, N_CTX)`, and is realised in the function `_attn_bwd_preprocess()`.\n",
    "\n",
    "2. Calculate $ dV, dK $ via the function `_attn_bwd_dkdv()`.\n",
    "\n",
    "3. Calculate $ dQ $ via the function `_attn_bwd_dq()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a7145bf-7a5c-41e5-b1f2-01724bb870c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _attn_bwd_preprocess(O, DO,  #\n",
    "                         Delta,  #\n",
    "                         Z, H, N_CTX,  #\n",
    "                         BLOCK_M: tl.constexpr, HEAD_DIM: tl.constexpr  #\n",
    "                         ):\n",
    "    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    off_hz = tl.program_id(1)\n",
    "    off_n = tl.arange(0, HEAD_DIM)\n",
    "    # load\n",
    "    o = tl.load(O + off_hz * HEAD_DIM * N_CTX + off_m[:, None] * HEAD_DIM + off_n[None, :]).to(tl.float32)\n",
    "    do = tl.load(DO + off_hz * HEAD_DIM * N_CTX + off_m[:, None] * HEAD_DIM + off_n[None, :]).to(tl.float32)\n",
    "    delta = tl.sum(o * do, axis=1)  \n",
    "    tl.store(Delta + off_hz * N_CTX + off_m, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1a2162-7da2-45b9-978b-ee851c070cf3",
   "metadata": {},
   "source": [
    "where `delta = tl.sum(o * do, axis=1)` implements the equation $ D_i = do_i^T o_i $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42029aae-c893-4f40-959a-1e86c1eefe72",
   "metadata": {},
   "source": [
    "To calculate $ dV, dK $, a block of elements of `k, v` is first loaded (sequence parallelisation), and then carries out a loop over the length dimension of `q`. \n",
    "\n",
    "```\n",
    "{\n",
    "    start_n = pid * BLOCK_N1\n",
    "    offs_n = start_n + tl.arange(0, BLOCK_N1)\n",
    "    # load K and V: they stay in SRAM throughout the inner loop.\n",
    "    k = tl.load(K + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n",
    "    v = tl.load(V + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n",
    "}\n",
    "```\n",
    "\n",
    "For the non-causal case, it is straightforward, \n",
    "\n",
    "```\n",
    "{\n",
    "start_m = 0\n",
    "num_steps = (N_CTX - start_m) // BLOCK_M1\n",
    "}\n",
    "```\n",
    "\n",
    "![alt](./figures/kq_dotprod_mat.png)\n",
    "\n",
    "For the causal case (please note that causal modelling is only used in self-attention), the procedure is split into two steps:\n",
    "\n",
    "1. Calculate the non-masked blocks (yellow squares in the above figure) by only changing `start_m = start_n + BLOCK_N1`.\n",
    "2. Calculate the diagonal block (the green square in the above figure) by setting\n",
    "   ```\n",
    "   {\n",
    "    start_m = start_n\n",
    "    MASK_BLOCK_M1: tl.constexpr = BLOCK_M1 // BLK_SLICE_FACTOR\n",
    "    num_steps = BLOCK_N1 // MASK_BLOCK_M1\n",
    "   }\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6808bcdb-0767-4b48-a748-c41967635e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main inner-loop logic for computing dK and dV.\n",
    "@triton.jit\n",
    "def _attn_bwd_dkdv(dk, dv,  #\n",
    "                   Q, k, v, sm_scale,  #\n",
    "                   DO,  #\n",
    "                   M, D,  #\n",
    "                   # shared by Q/K/V/DO.\n",
    "                   stride_tok, stride_d,  #\n",
    "                   H, N_CTX, BLOCK_M1: tl.constexpr,  #\n",
    "                   BLOCK_N1: tl.constexpr,  #\n",
    "                   HEAD_DIM: tl.constexpr,  #\n",
    "                   # Filled in by the wrapper.\n",
    "                   start_n, start_m, num_steps,  #\n",
    "                   MASK: tl.constexpr):\n",
    "    offs_m = start_m + tl.arange(0, BLOCK_M1)\n",
    "    offs_n = start_n + tl.arange(0, BLOCK_N1)\n",
    "    offs_k = tl.arange(0, HEAD_DIM)\n",
    "    qT_ptrs = Q + offs_m[None, :] * stride_tok + offs_k[:, None] * stride_d\n",
    "    do_ptrs = DO + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n",
    "    # BLOCK_N1 must be a multiple of BLOCK_M1, otherwise the code wouldn't work.\n",
    "    tl.static_assert(BLOCK_N1 % BLOCK_M1 == 0)\n",
    "    curr_m = start_m\n",
    "    step_m = BLOCK_M1\n",
    "    for blk_idx in range(num_steps):\n",
    "        qT = tl.load(qT_ptrs)\n",
    "        # Load m before computing qk to reduce pipeline stall.\n",
    "        offs_m = curr_m + tl.arange(0, BLOCK_M1)\n",
    "        m = tl.load(M + offs_m)\n",
    "        sT = tl.dot(k, qT)\n",
    "        pT = tl.math.exp2(sT - m[None, :])\n",
    "        # Autoregressive masking.\n",
    "        if MASK:\n",
    "            mask = (offs_m[None, :] >= offs_n[:, None])\n",
    "            pT = tl.where(mask, pT, 0.0)\n",
    "        do = tl.load(do_ptrs)\n",
    "        # Compute dV.\n",
    "        ppT = pT\n",
    "        ppT = ppT.to(tl.float16)\n",
    "        dv += tl.dot(ppT, do)\n",
    "        # D (= delta) is pre-divided by ds_scale.\n",
    "        Di = tl.load(D + offs_m)\n",
    "        # Compute dP and dS.\n",
    "        dpT = tl.dot(v, tl.trans(do)).to(tl.float32)\n",
    "        dsT = pT * (dpT - Di[None, :])\n",
    "        dsT = dsT.to(tl.float16)\n",
    "        dk += tl.dot(dsT, tl.trans(qT))\n",
    "        # Increment pointers.\n",
    "        curr_m += step_m\n",
    "        qT_ptrs += step_m * stride_tok\n",
    "        do_ptrs += step_m * stride_tok\n",
    "    return dk, dv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e52688d-7e7f-437d-a3d2-15baddec9af0",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    " qT = tl.load(qT_ptrs)\n",
    " # Load m before computing qk to reduce pipeline stall.\n",
    " offs_m = curr_m + tl.arange(0, BLOCK_M1)\n",
    " m = tl.load(M + offs_m)\n",
    " sT = tl.dot(k, qT)\n",
    " pT = tl.math.exp2(sT - m[None, :])\n",
    "}\n",
    "```\n",
    "\n",
    "This part of code recomputes $ S = QK^T $ and $ P = \\operatorname{softmax}(S) $ (actually its transposed version, and therefore it needs to pay attention to the broadcast rule `m[None, :]`. `m` is stored in the forward pass for calculating softmax in a numerical stable manner.).\n",
    "\n",
    "`dv += tl.dot(ppT, do)` implements the equation $ dV = P^T dO $. As the calculation $ dv_j = \\sum_i P_{ij} do_i $, where $ dv_j, do_i $ denote the j-th and i-th row of $ V, O $ respectively, is chunked into multiple blocks, so do not forget the accumulation sum.\n",
    "\n",
    "`dpT = tl.dot(v, tl.trans(do)).to(tl.float32)` implements the equation $ dP = dO V^T $ (its transposed version).\n",
    "\n",
    "`dsT = pT * (dpT - Di[None, :])` implements the equation $ dS = \\operatorname{dsoftmax}(dP) \\in \\mathbb{R}^{N\\times N} $, which is further simplified to $ dS_{i,:} = \\operatorname{dsoftmax}dP_{i,:} = (\\text{diag}(P_{i,:}) - P_{i,:}P_{i,:}^T)dP_{i,:} = P_{i,:} \\circ dP_{i,:} - \\left( P_{i,:}^T dP_{i,:} \\right) P_{i,:} = P_{i,:} \\circ dP_{i,:} - D_i P_{i,:} $ as discussed above (its transposed version).\n",
    "\n",
    "`dk += tl.dot(dsT, tl.trans(qT))` implements the equation $ dK = dS^T Q $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c028d2-76f2-4660-aad5-fee33445b31e",
   "metadata": {},
   "source": [
    "$ dQ $ is calculated similarly: a block of elements of `q` is first loaded (sequence parallelisation), and then carries out a loop over the length dimension of `k, v`.\n",
    "\n",
    "```\n",
    "{\n",
    " start_m = pid * BLOCK_M2\n",
    " offs_m = start_m + tl.arange(0, BLOCK_M2)\n",
    " # load q, do, m and Di: they stay in SRAM throughout the inner loop.\n",
    " q = tl.load(Q + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d)\n",
    " do = tl.load(DO + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d)\n",
    "\n",
    " m = tl.load(M + offs_m)\n",
    " m = m[:, None]\n",
    "\n",
    " Di = tl.load(D + offs_m)\n",
    " Di = Di[:, None]\n",
    "}\n",
    "```\n",
    "\n",
    "![alt](./figures/qk_dotprod_mat.png)\n",
    "\n",
    "For the causal case, the procedure is split into two steps:\n",
    "\n",
    "1. Calculate the non-masked blocks (yellow squares in the above figure) by setting\n",
    "   ```\n",
    "   {\n",
    "    end_n = start_m\n",
    "    num_steps = end_n // BLOCK_N2\n",
    "   }\n",
    "   ```\n",
    "   So in the inner loop over `k, v`, the start and end indexes are `0` and `end_n = start_m`, respectively.\n",
    "2. Calculate the diagonal block (the green square in the above figure) by setting\n",
    "   ```\n",
    "   {\n",
    "    MASK_BLOCK_N2: tl.constexpr = BLOCK_N2 // BLK_SLICE_FACTOR\n",
    "    num_steps = BLOCK_M2 // MASK_BLOCK_N2\n",
    "   }\n",
    "   ```\n",
    "   And the start and end indexes are `start_m` and `start_m + BLOCK_M2` respectively.\n",
    "\n",
    "For the non-causal case, in the inner loop over `k, v`, the start and end indexes are simply `0` and `N_CTX`, respectively. However, in my implementation, it is also split into two steps: 1) from `0` to `start_m`, and 2) from `start_m` to `N_CTX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af7c9765-6df8-489a-832b-a36d4db760fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _attn_bwd_dq(dq, q, K, V,  #\n",
    "                 do, m, Di,\n",
    "                 # shared by Q/K/V/DO.\n",
    "                 stride_tok, stride_d,  #\n",
    "                 H, N_CTX,  #\n",
    "                 BLOCK_M2: tl.constexpr,  #\n",
    "                 BLOCK_N2: tl.constexpr,  #\n",
    "                 HEAD_DIM: tl.constexpr,  #\n",
    "                 # Filled in by the wrapper.\n",
    "                 start_m, start_n, num_steps,  #\n",
    "                 MASK: tl.constexpr):\n",
    "    offs_m = start_m + tl.arange(0, BLOCK_M2)\n",
    "    offs_n = start_n + tl.arange(0, BLOCK_N2)\n",
    "    offs_k = tl.arange(0, HEAD_DIM)\n",
    "    kT_ptrs = K + offs_n[None, :] * stride_tok + offs_k[:, None] * stride_d\n",
    "    vT_ptrs = V + offs_n[None, :] * stride_tok + offs_k[:, None] * stride_d\n",
    "    # BLOCK_M2 must be a multiple of BLOCK_N2, otherwise the code wouldn't work.\n",
    "    tl.static_assert(BLOCK_M2 % BLOCK_N2 == 0)\n",
    "    curr_n = start_n\n",
    "    step_n = BLOCK_N2\n",
    "    for blk_idx in range(num_steps):\n",
    "        kT = tl.load(kT_ptrs)\n",
    "        vT = tl.load(vT_ptrs)\n",
    "        s = tl.dot(q, kT)\n",
    "        p = tl.math.exp2(s - m)\n",
    "        # Autoregressive masking.\n",
    "        if MASK:\n",
    "            offs_n = curr_n + tl.arange(0, BLOCK_N2)\n",
    "            mask = (offs_m[:, None] >= offs_n[None, :])\n",
    "            p = tl.where(mask, p, 0.0)\n",
    "        # Compute dP and dS.\n",
    "        dp = tl.dot(do, vT).to(tl.float32)\n",
    "        ds = p * (dp - Di)\n",
    "        ds = ds.to(tl.float16)\n",
    "        # Compute dQ.\n",
    "        # NOTE: We need to de-scale dq in the end, because kT was pre-scaled.\n",
    "        dq += tl.dot(ds, tl.trans(kT))\n",
    "        # Increment pointers.\n",
    "        curr_n += step_n\n",
    "        kT_ptrs += step_n * stride_tok\n",
    "        vT_ptrs += step_n * stride_tok\n",
    "    return dq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ceda6d-f3a2-4361-873f-a9861bb71850",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    " kT = tl.load(kT_ptrs)\n",
    " vT = tl.load(vT_ptrs)\n",
    " s = tl.dot(q, kT)\n",
    " p = tl.math.exp2(s - m)\n",
    "}\n",
    "```\n",
    "\n",
    "This part of code recomputes $ S = QK^T $ and $ P = \\operatorname{softmax}(S) $.\n",
    "\n",
    "`dp = tl.dot(do, vT).to(tl.float32)` implements the equation $ dP = dO V^T $.\n",
    "\n",
    "`ds = p * (dp - Di)` implements the equation $ dS_{i,:} = P_{i,:} \\circ dP_{i,:} - D_i P_{i,:} $.\n",
    "\n",
    "`dq += tl.dot(ds, tl.trans(kT))` implements the equation $ dQ = dS K $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037743ff-87d2-4132-84b1-882dc1a90d69",
   "metadata": {},
   "source": [
    "## Concluding Remarks\n",
    "\n",
    "Voila! We have walked through the core implementation of the backward pass of FlashAttention, where the Triton code exhibits a high similarity to matrix calculus equations. The following code is a full implementation of FlashAttention2 in Triton. However, for pedagogical purposes, it is a restricted version that cannot handle arbitrary lengths. You can also check another IPython notebook, *FlashAttention Triton Implementation*, where I provide a more flexible implementation of FlashAttention2 that can handle both self-attention and cross-attention with arbitrary sequence lengths. For practical usage, I recommend using the official [FlashAttention Repo](https://github.com/Dao-AILab/flash-attention) written in CUDA or refer to its Triton implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8122dc45-d961-4588-bd8e-cd46d3a0a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "\n",
    "def is_cuda():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _attn_fwd_inner(acc, l_i, m_i, q,  #\n",
    "                    kT_ptrs, v_ptrs,  #\n",
    "                    start_m, qk_scale,  #\n",
    "                    BLOCK_M: tl.constexpr, HEAD_DIM: tl.constexpr, BLOCK_N: tl.constexpr,  #\n",
    "                    STAGE: tl.constexpr, offs_m: tl.constexpr, offs_n: tl.constexpr,  #\n",
    "                    N_CTX: tl.constexpr):\n",
    "    # range of values handled by this stage\n",
    "    if STAGE == 1:\n",
    "        lo, hi = 0, start_m * BLOCK_M\n",
    "    elif STAGE == 2:\n",
    "        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n",
    "        lo = tl.multiple_of(lo, BLOCK_M)\n",
    "    # causal = False\n",
    "    else:\n",
    "        lo, hi = 0, N_CTX\n",
    "    kT_ptrs += lo * HEAD_DIM\n",
    "    v_ptrs += lo * HEAD_DIM\n",
    "    # loop over k, v and update accumulator\n",
    "    for start_n in range(lo, hi, BLOCK_N):\n",
    "        start_n = tl.multiple_of(start_n, BLOCK_N)\n",
    "        # -- compute qk ----\n",
    "        kT = tl.load(kT_ptrs)\n",
    "        s = tl.dot(q, kT)\n",
    "        if STAGE == 2:\n",
    "            mask = offs_m[:, None] >= (start_n + offs_n[None, :])\n",
    "            s = s * qk_scale + tl.where(mask, 0, -1.0e6)\n",
    "            m_ij = tl.maximum(m_i, tl.max(s, 1))\n",
    "            s -= m_ij[:, None]\n",
    "        else:\n",
    "            m_ij = tl.maximum(m_i, tl.max(s, 1) * qk_scale)\n",
    "            s = s * qk_scale - m_ij[:, None]\n",
    "        p = tl.math.exp2(s)\n",
    "        l_ij = tl.sum(p, 1)\n",
    "        # -- update m_i and l_i\n",
    "        alpha = tl.math.exp2(m_i - m_ij)\n",
    "        l_i = l_i * alpha + l_ij\n",
    "        # -- update output accumulator --\n",
    "        acc = acc * alpha[:, None]\n",
    "        # update acc\n",
    "        v = tl.load(v_ptrs)\n",
    "        p = p.to(tl.float16)\n",
    "        acc = tl.dot(p, v, acc)\n",
    "        # update m_i and l_i\n",
    "        m_i = m_ij\n",
    "        kT_ptrs += BLOCK_N * HEAD_DIM\n",
    "        v_ptrs += BLOCK_N * HEAD_DIM\n",
    "    return acc, l_i, m_i\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _attn_fwd(Q, K, V, sm_scale, M, Out,  #\n",
    "              stride_qz, stride_qh, stride_qm, stride_qk,  #\n",
    "              stride_kz, stride_kh, stride_kn, stride_kk,  #\n",
    "              stride_vz, stride_vh, stride_vn, stride_vk,  #\n",
    "              stride_oz, stride_oh, stride_om, stride_ok,  #\n",
    "              Z, H, N_CTX,  #\n",
    "              HEAD_DIM: tl.constexpr,  #\n",
    "              BLOCK_M: tl.constexpr,  #\n",
    "              BLOCK_N: tl.constexpr,  #\n",
    "              STAGE: tl.constexpr  #\n",
    "              ):\n",
    "    tl.static_assert(BLOCK_N <= HEAD_DIM)\n",
    "    start_m = tl.program_id(0)\n",
    "    off_hz = tl.program_id(1)\n",
    "    off_z = off_hz // H\n",
    "    off_h = off_hz % H\n",
    "    qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n",
    "\n",
    "    # offset pointers for batch/head\n",
    "    Q += qvk_offset\n",
    "    K += qvk_offset\n",
    "    V += qvk_offset\n",
    "    Out += qvk_offset\n",
    "    \n",
    "    # initialize offsets\n",
    "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    offs_n = tl.arange(0, BLOCK_N)\n",
    "    offs_k = tl.arange(0, HEAD_DIM)\n",
    "    # initialize pointer to m and l\n",
    "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n",
    "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n",
    "    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\n",
    "    # load scales\n",
    "    qk_scale = sm_scale\n",
    "    qk_scale *= 1.44269504  # 1/log(2)\n",
    "    # load q: it will stay in SRAM throughout\n",
    "    q_ptrs = Q + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk \n",
    "    q = tl.load(q_ptrs)\n",
    "\n",
    "    kT_ptrs = K + offs_n[None, :] * stride_kn + offs_k[:, None] * stride_kk\n",
    "    v_ptrs =  V + offs_n[:, None] * stride_vn + offs_k[None, :] * stride_vk\n",
    "    \n",
    "    # stage 1: off-band\n",
    "    # For causal = True, STAGE = 3 and _attn_fwd_inner gets 1 as its STAGE\n",
    "    # For causal = False, STAGE = 1, and _attn_fwd_inner gets 3 as its STAGE\n",
    "    if STAGE & 1:\n",
    "        acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, kT_ptrs, v_ptrs,  #\n",
    "                                        start_m, qk_scale,  #\n",
    "                                        BLOCK_M, HEAD_DIM, BLOCK_N,  #\n",
    "                                        4 - STAGE, offs_m, offs_n, N_CTX  #\n",
    "                                        )\n",
    "    # stage 2: on-band\n",
    "    if STAGE & 2:\n",
    "        # barrier makes it easier for compielr to schedule the\n",
    "        # two loops independently\n",
    "        acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, kT_ptrs, v_ptrs,  #\n",
    "                                        start_m, qk_scale,  #\n",
    "                                        BLOCK_M, HEAD_DIM, BLOCK_N,  #\n",
    "                                        2, offs_m, offs_n, N_CTX  #\n",
    "                                        )\n",
    "    # epilogue\n",
    "    m_i += tl.math.log2(l_i)\n",
    "    acc = acc / l_i[:, None]\n",
    "    m_ptrs = M + off_hz * N_CTX + offs_m\n",
    "    tl.store(m_ptrs, m_i)\n",
    "    o_ptrs = Out + offs_m[:, None] * stride_om + offs_k[None, :] * stride_ok\n",
    "    tl.store(o_ptrs, acc.to(Out.type.element_ty))\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _attn_bwd_preprocess(O, DO,  #\n",
    "                         Delta,  #\n",
    "                         Z, H, N_CTX,  #\n",
    "                         BLOCK_M: tl.constexpr, HEAD_DIM: tl.constexpr  #\n",
    "                         ):\n",
    "    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    off_hz = tl.program_id(1)\n",
    "    off_n = tl.arange(0, HEAD_DIM)\n",
    "    # load\n",
    "    o = tl.load(O + off_hz * HEAD_DIM * N_CTX + off_m[:, None] * HEAD_DIM + off_n[None, :]).to(tl.float32)\n",
    "    do = tl.load(DO + off_hz * HEAD_DIM * N_CTX + off_m[:, None] * HEAD_DIM + off_n[None, :]).to(tl.float32)\n",
    "    delta = tl.sum(o * do, axis=1)\n",
    "    # write-back\n",
    "    tl.store(Delta + off_hz * N_CTX + off_m, delta)\n",
    "\n",
    "\n",
    "# The main inner-loop logic for computing dK and dV.\n",
    "@triton.jit\n",
    "def _attn_bwd_dkdv(dk, dv,  #\n",
    "                   Q, k, v, sm_scale,  #\n",
    "                   DO,  #\n",
    "                   M, D,  #\n",
    "                   # shared by Q/K/V/DO.\n",
    "                   stride_tok, stride_d,  #\n",
    "                   H, N_CTX, BLOCK_M1: tl.constexpr,  #\n",
    "                   BLOCK_N1: tl.constexpr,  #\n",
    "                   HEAD_DIM: tl.constexpr,  #\n",
    "                   # Filled in by the wrapper.\n",
    "                   start_n, start_m, num_steps,  #\n",
    "                   MASK: tl.constexpr):\n",
    "    offs_m = start_m + tl.arange(0, BLOCK_M1)\n",
    "    offs_n = start_n + tl.arange(0, BLOCK_N1)\n",
    "    offs_k = tl.arange(0, HEAD_DIM)\n",
    "    qT_ptrs = Q + offs_m[None, :] * stride_tok + offs_k[:, None] * stride_d\n",
    "    do_ptrs = DO + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n",
    "    # BLOCK_N1 must be a multiple of BLOCK_M1, otherwise the code wouldn't work.\n",
    "    tl.static_assert(BLOCK_N1 % BLOCK_M1 == 0)\n",
    "    curr_m = start_m\n",
    "    step_m = BLOCK_M1\n",
    "    for blk_idx in range(num_steps):\n",
    "        qT = tl.load(qT_ptrs)\n",
    "        # Load m before computing qk to reduce pipeline stall.\n",
    "        offs_m = curr_m + tl.arange(0, BLOCK_M1)\n",
    "        m = tl.load(M + offs_m)\n",
    "        sT = tl.dot(k, qT)\n",
    "        pT = tl.math.exp2(sT - m[None, :])\n",
    "        # Autoregressive masking.\n",
    "        if MASK:\n",
    "            mask = (offs_m[None, :] >= offs_n[:, None])\n",
    "            pT = tl.where(mask, pT, 0.0)\n",
    "        do = tl.load(do_ptrs)\n",
    "        # Compute dV.\n",
    "        ppT = pT\n",
    "        ppT = ppT.to(tl.float16)\n",
    "        dv += tl.dot(ppT, do)\n",
    "        # D (= delta) is pre-divided by ds_scale.\n",
    "        Di = tl.load(D + offs_m)\n",
    "        # Compute dP and dS.\n",
    "        dpT = tl.dot(v, tl.trans(do)).to(tl.float32)\n",
    "        dsT = pT * (dpT - Di[None, :])\n",
    "        dsT = dsT.to(tl.float16)\n",
    "        dk += tl.dot(dsT, tl.trans(qT))\n",
    "        # Increment pointers.\n",
    "        curr_m += step_m\n",
    "        qT_ptrs += step_m * stride_tok\n",
    "        do_ptrs += step_m * stride_tok\n",
    "    return dk, dv\n",
    "\n",
    "\n",
    "# the main inner-loop logic for computing dQ\n",
    "@triton.jit\n",
    "def _attn_bwd_dq(dq, q, K, V,  #\n",
    "                 do, m, Di,\n",
    "                 # shared by Q/K/V/DO.\n",
    "                 stride_tok, stride_d,  #\n",
    "                 H, N_CTX,  #\n",
    "                 BLOCK_M2: tl.constexpr,  #\n",
    "                 BLOCK_N2: tl.constexpr,  #\n",
    "                 HEAD_DIM: tl.constexpr,  #\n",
    "                 # Filled in by the wrapper.\n",
    "                 start_m, start_n, num_steps,  #\n",
    "                 MASK: tl.constexpr):\n",
    "    offs_m = start_m + tl.arange(0, BLOCK_M2)\n",
    "    offs_n = start_n + tl.arange(0, BLOCK_N2)\n",
    "    offs_k = tl.arange(0, HEAD_DIM)\n",
    "    kT_ptrs = K + offs_n[None, :] * stride_tok + offs_k[:, None] * stride_d\n",
    "    vT_ptrs = V + offs_n[None, :] * stride_tok + offs_k[:, None] * stride_d\n",
    "    # BLOCK_M2 must be a multiple of BLOCK_N2, otherwise the code wouldn't work.\n",
    "    tl.static_assert(BLOCK_M2 % BLOCK_N2 == 0)\n",
    "    curr_n = start_n\n",
    "    step_n = BLOCK_N2\n",
    "    for blk_idx in range(num_steps):\n",
    "        kT = tl.load(kT_ptrs)\n",
    "        vT = tl.load(vT_ptrs)\n",
    "        s = tl.dot(q, kT)\n",
    "        p = tl.math.exp2(s - m)\n",
    "        # Autoregressive masking.\n",
    "        if MASK:\n",
    "            offs_n = curr_n + tl.arange(0, BLOCK_N2)\n",
    "            mask = (offs_m[:, None] >= offs_n[None, :])\n",
    "            p = tl.where(mask, p, 0.0)\n",
    "        # Compute dP and dS.\n",
    "        dp = tl.dot(do, vT).to(tl.float32)\n",
    "        ds = p * (dp - Di)\n",
    "        ds = ds.to(tl.float16)\n",
    "        # Compute dQ.\n",
    "        # NOTE: We need to de-scale dq in the end, because kT was pre-scaled.\n",
    "        dq += tl.dot(ds, tl.trans(kT))\n",
    "        # Increment pointers.\n",
    "        curr_n += step_n\n",
    "        kT_ptrs += step_n * stride_tok\n",
    "        vT_ptrs += step_n * stride_tok\n",
    "    return dq\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _attn_bwd(Q, K, V, sm_scale,  #\n",
    "              DO,  #\n",
    "              DQ, DK, DV,  #\n",
    "              M, D,\n",
    "              # shared by Q/K/V/DO. (a simplified version)\n",
    "              stride_z, stride_h, stride_tok, stride_d,  #\n",
    "              H, N_CTX,  #\n",
    "              BLOCK_M1: tl.constexpr,  #\n",
    "              BLOCK_N1: tl.constexpr,  #\n",
    "              BLOCK_M2: tl.constexpr,  #\n",
    "              BLOCK_N2: tl.constexpr,  #\n",
    "              BLK_SLICE_FACTOR: tl.constexpr,  #\n",
    "              HEAD_DIM: tl.constexpr,  #\n",
    "              STAGE: tl.constexpr,  #\n",
    "             ):\n",
    "    LN2: tl.constexpr = 0.6931471824645996  # = ln(2)\n",
    "\n",
    "    tl.static_assert(BLOCK_N1 % BLOCK_M1 == 0)\n",
    "    tl.static_assert(BLOCK_M2 % BLOCK_N2 == 0)\n",
    "    \n",
    "    bhid = tl.program_id(2)\n",
    "    off_chz = (bhid * N_CTX).to(tl.int64)\n",
    "    adj = (stride_h * (bhid % H) + stride_z * (bhid // H)).to(tl.int64)\n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    # offset pointers for batch/head\n",
    "    Q += adj\n",
    "    K += adj\n",
    "    V += adj\n",
    "    DO += adj\n",
    "    DQ += adj\n",
    "    DK += adj\n",
    "    DV += adj\n",
    "    M += off_chz\n",
    "    D += off_chz\n",
    "\n",
    "    # load scales\n",
    "    offs_k = tl.arange(0, HEAD_DIM)\n",
    "\n",
    "    start_n = pid * BLOCK_N1\n",
    "    offs_n = start_n + tl.arange(0, BLOCK_N1)\n",
    "\n",
    "    dv = tl.zeros([BLOCK_N1, HEAD_DIM], dtype=tl.float32)\n",
    "    dk = tl.zeros([BLOCK_N1, HEAD_DIM], dtype=tl.float32)\n",
    "\n",
    "    # load K and V: they stay in SRAM throughout the inner loop.\n",
    "    k = tl.load(K + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n",
    "    v = tl.load(V + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n",
    "\n",
    "    # For causal = True, STAGE = 3 \n",
    "    # For causal = False, STAGE = 1\n",
    "    # Compute dK and dV for non-masked blocks.\n",
    "    start_m = start_n + BLOCK_N1 if STAGE == 3 else 0\n",
    "    num_steps = (N_CTX - start_m) // BLOCK_M1\n",
    "\n",
    "    dk, dv = _attn_bwd_dkdv(dk, dv,  #\n",
    "                            Q, k, v, sm_scale,  #\n",
    "                            DO,  #\n",
    "                            M, D,  #\n",
    "                            stride_tok, stride_d,  #\n",
    "                            H, N_CTX,  #\n",
    "                            BLOCK_M1, BLOCK_N1, HEAD_DIM,  #\n",
    "                            start_n, start_m, num_steps,  #\n",
    "                            MASK=False  #\n",
    "                            )\n",
    "    \n",
    "    if STAGE & 2: # diagonal block for causal masking\n",
    "        start_m = start_n\n",
    "        MASK_BLOCK_M1: tl.constexpr = BLOCK_M1 // BLK_SLICE_FACTOR\n",
    "        num_steps = BLOCK_N1 // MASK_BLOCK_M1\n",
    "\n",
    "        # Compute dK and dV for non-masked blocks.\n",
    "        dk, dv = _attn_bwd_dkdv(  #\n",
    "                                dk, dv,  #\n",
    "                                Q, k, v, sm_scale,  #\n",
    "                                DO,  #\n",
    "                                M, D,  #\n",
    "                                stride_tok, stride_d,  #\n",
    "                                H, N_CTX,  #\n",
    "                                MASK_BLOCK_M1, BLOCK_N1, HEAD_DIM,  #\n",
    "                                start_n, start_m, num_steps,  #\n",
    "                                MASK=True  #\n",
    "                                )\n",
    "\n",
    "    dv_ptrs = DV + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d\n",
    "    tl.store(dv_ptrs, dv)\n",
    "\n",
    "    # Write back dK.\n",
    "    dk *= sm_scale\n",
    "    dk_ptrs = DK + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d\n",
    "    tl.store(dk_ptrs, dk)\n",
    "\n",
    "    # THIS BLOCK DOES DQ:\n",
    "    start_m = pid * BLOCK_M2\n",
    "    offs_m = start_m + tl.arange(0, BLOCK_M2)\n",
    "    \n",
    "    q = tl.load(Q + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d)\n",
    "    dq = tl.zeros([BLOCK_M2, HEAD_DIM], dtype=tl.float32)\n",
    "    do = tl.load(DO + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d)\n",
    "\n",
    "    m = tl.load(M + offs_m)\n",
    "    m = m[:, None]\n",
    "\n",
    "    Di = tl.load(D + offs_m)\n",
    "    Di = Di[:, None]\n",
    "\n",
    "    end_n = start_m\n",
    "    num_steps = end_n // BLOCK_N2\n",
    "\n",
    "    dq = _attn_bwd_dq(dq, q, K, V,  #\n",
    "                      do, m, Di,  #\n",
    "                      stride_tok, stride_d,  #\n",
    "                      H, N_CTX,  #\n",
    "                      BLOCK_M2, BLOCK_N2, HEAD_DIM,  #\n",
    "                      start_m, 0, num_steps,  #\n",
    "                      MASK=False  #\n",
    "                      )\n",
    "\n",
    "    if STAGE & 2:\n",
    "        # Compute dQ for masked (diagonal) blocks when using causal masking\n",
    "        MASK_BLOCK_N2: tl.constexpr = BLOCK_N2 // BLK_SLICE_FACTOR\n",
    "        num_steps = BLOCK_M2 // MASK_BLOCK_N2\n",
    "        \n",
    "        dq = _attn_bwd_dq(dq, q, K, V,  #\n",
    "                          do, m, Di,  #\n",
    "                          stride_tok, stride_d,  #\n",
    "                          H, N_CTX,  #\n",
    "                          BLOCK_M2, MASK_BLOCK_N2, HEAD_DIM,  #\n",
    "                          start_m, start_m, num_steps,  #\n",
    "                          MASK=True  #\n",
    "                          )\n",
    "    else:\n",
    "        end_n = N_CTX - start_m\n",
    "        num_steps = end_n // BLOCK_N2\n",
    "        dq = _attn_bwd_dq(dq, q, K, V,  #\n",
    "                          do, m, Di,  #\n",
    "                          stride_tok, stride_d,  #\n",
    "                          H, N_CTX,  #\n",
    "                          BLOCK_M2, BLOCK_N2, HEAD_DIM,  #\n",
    "                          start_m, start_m, num_steps,  #\n",
    "                          MASK=False  #\n",
    "                          )\n",
    "    # Write back dQ.\n",
    "    dq_ptrs = DQ + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n",
    "    dq *= LN2\n",
    "    tl.store(dq_ptrs, dq)\n",
    "\n",
    "\n",
    "class _attention(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, q, k, v, causal, sm_scale):\n",
    "        # shape constraints\n",
    "        HEAD_DIM_Q, HEAD_DIM_K = q.shape[-1], k.shape[-1]\n",
    "        HEAD_DIM_V = v.shape[-1]\n",
    "        N_CTX = q.shape[-2]\n",
    "        assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n",
    "        assert HEAD_DIM_K in {16, 32, 64, 128, 256}\n",
    "        o = torch.empty_like(q)\n",
    "        stage = 3 if causal else 1\n",
    "        extra_kern_args = {}\n",
    "\n",
    "        BLOCK_M, BLOCK_N = 64, 64\n",
    "        # a restricted version that cannot handle arbitrary length for illustrated purposes only\n",
    "        assert N_CTX % BLOCK_M == 0\n",
    "        assert N_CTX % BLOCK_N == 0\n",
    "        \n",
    "        M = torch.empty((q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n",
    "       \n",
    "        grid = lambda args: (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n",
    "        ctx.grid = grid\n",
    "        _attn_fwd[grid](\n",
    "            q, k, v, sm_scale, M, o,  #\n",
    "            q.stride(0), q.stride(1), q.stride(2), q.stride(3),  #\n",
    "            k.stride(0), k.stride(1), k.stride(2), k.stride(3),  #\n",
    "            v.stride(0), v.stride(1), v.stride(2), v.stride(3),  #\n",
    "            o.stride(0), o.stride(1), o.stride(2), o.stride(3),  #\n",
    "            q.shape[0], q.shape[1],  #\n",
    "            N_CTX=q.shape[2],  #\n",
    "            HEAD_DIM=HEAD_DIM_K,  #\n",
    "            BLOCK_M=BLOCK_M,  #\n",
    "            BLOCK_N=BLOCK_N,  #\n",
    "            STAGE=stage,  #\n",
    "            **extra_kern_args)\n",
    "\n",
    "        ctx.save_for_backward(q, k, v, o, M)\n",
    "        ctx.sm_scale = sm_scale\n",
    "        ctx.HEAD_DIM = HEAD_DIM_K\n",
    "        ctx.causal = causal\n",
    "        return o\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, do):\n",
    "        q, k, v, o, M = ctx.saved_tensors\n",
    "        assert do.is_contiguous()\n",
    "        assert q.stride() == k.stride() == v.stride() == o.stride() == do.stride()\n",
    "        dq = torch.empty_like(q)\n",
    "        dk = torch.empty_like(k)\n",
    "        dv = torch.empty_like(v)\n",
    "        BATCH, N_HEAD, N_CTX = q.shape[:3]\n",
    "        PRE_BLOCK = 128\n",
    "        # NUM_WARPS, NUM_STAGES = 4, 5\n",
    "        BLOCK_M1, BLOCK_N1, BLOCK_M2, BLOCK_N2 = 32, 128, 128, 32\n",
    "        BLK_SLICE_FACTOR = 2\n",
    "        RCP_LN2 = 1.4426950408889634  # = 1.0 / ln(2)\n",
    "        arg_k = k\n",
    "        arg_k = arg_k * (ctx.sm_scale * RCP_LN2)\n",
    "        PRE_BLOCK = 128\n",
    "        assert N_CTX % PRE_BLOCK == 0\n",
    "        pre_grid = (N_CTX // PRE_BLOCK, BATCH * N_HEAD)\n",
    "        delta = torch.empty_like(M)\n",
    "        _attn_bwd_preprocess[pre_grid](\n",
    "            o, do,  #\n",
    "            delta,  #\n",
    "            BATCH, N_HEAD, N_CTX,  #\n",
    "            BLOCK_M=PRE_BLOCK, HEAD_DIM=ctx.HEAD_DIM  #\n",
    "        )\n",
    "        grid = (N_CTX // BLOCK_N1, 1, BATCH * N_HEAD)\n",
    "        _attn_bwd[grid](\n",
    "            q, arg_k, v, ctx.sm_scale, do, dq, dk, dv,  #\n",
    "            M, delta,  #\n",
    "            q.stride(0), q.stride(1), q.stride(2), q.stride(3),  #\n",
    "            N_HEAD, N_CTX,  #\n",
    "            BLOCK_M1=BLOCK_M1, BLOCK_N1=BLOCK_N1,  #\n",
    "            BLOCK_M2=BLOCK_M2, BLOCK_N2=BLOCK_N2,  #\n",
    "            BLK_SLICE_FACTOR=BLK_SLICE_FACTOR,  #\n",
    "            HEAD_DIM=ctx.HEAD_DIM,  #\n",
    "            STAGE=3 if ctx.causal else 1  #\n",
    "        )\n",
    "\n",
    "        return dq, dk, dv, None, None, None, None\n",
    "\n",
    "\n",
    "attention = _attention.apply\n",
    "\n",
    "\n",
    "def test_op(Z, H, N_CTX, HEAD_DIM, causal=False, dtype=torch.float16):\n",
    "    torch.manual_seed(20)\n",
    "    q = (torch.empty((Z, H, N_CTX, HEAD_DIM), dtype=dtype, device=DEVICE).normal_(mean=0.0, std=0.5).requires_grad_())\n",
    "    k = (torch.empty((Z, H, N_CTX, HEAD_DIM), dtype=dtype, device=DEVICE).normal_(mean=0.0, std=0.5).requires_grad_())\n",
    "    v = (torch.empty((Z, H, N_CTX, HEAD_DIM), dtype=dtype, device=DEVICE).normal_(mean=0.0, std=0.5).requires_grad_())\n",
    "    sm_scale = 0.5\n",
    "    dout = torch.randn_like(q)\n",
    "    # reference implementation\n",
    "    M = torch.tril(torch.ones((N_CTX, N_CTX), device=DEVICE))\n",
    "    p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n",
    "    if causal:\n",
    "        p[:, :, M == 0] = float(\"-inf\")\n",
    "    p = torch.softmax(p.float(), dim=-1).half()\n",
    "    # p = torch.exp(p)\n",
    "    ref_out = torch.matmul(p, v)\n",
    "    ref_out.backward(dout)\n",
    "    ref_dv, v.grad = v.grad.clone(), None\n",
    "    ref_dk, k.grad = k.grad.clone(), None\n",
    "    ref_dq, q.grad = q.grad.clone(), None\n",
    "    # triton implementation\n",
    "    tri_out = attention(q, k, v, causal, sm_scale).half()\n",
    "    tri_out.backward(dout)\n",
    "    tri_dv, v.grad = v.grad.clone(), None\n",
    "    tri_dk, k.grad = k.grad.clone(), None\n",
    "    tri_dq, q.grad = q.grad.clone(), None\n",
    "    # compare\n",
    "    torch.testing.assert_close(ref_out, tri_out, atol=1e-2, rtol=0)\n",
    "    rtol = 0.0\n",
    "    # Relative tolerance workaround for known hardware limitation of CDNA2 GPU.\n",
    "    # For details see https://pytorch.org/docs/stable/notes/numerical_accuracy.html#reduced-precision-fp16-and-bf16-gemms-and-convolutions-on-amd-instinct-mi200-devices\n",
    "    if torch.version.hip is not None and triton.runtime.driver.active.get_current_target().arch == \"gfx90a\":\n",
    "        rtol = 1e-2\n",
    "    torch.testing.assert_close(ref_dv, tri_dv, atol=1e-2, rtol=rtol)\n",
    "    torch.testing.assert_close(ref_dk, tri_dk, atol=1e-2, rtol=rtol)\n",
    "    torch.testing.assert_close(ref_dq, tri_dq, atol=1e-2, rtol=rtol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2634360-ad20-42f7-8dd9-b0f2e76deccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xcai/.conda/envs/llm_reason/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /opt/conda/conda-bld/pytorch_1729647348947/work/aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "# As the above code is a restricted version that cannot handle arbitrary lengths for illustrated purposes only\n",
    "# N_CTX needs to be a multiple of 32\n",
    "\n",
    "test_op(1, 2, 1024, 64, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd51d01-1829-48b6-923c-4d19e1d835cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
